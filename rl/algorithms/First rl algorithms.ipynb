{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989ce86b",
   "metadata": {},
   "source": [
    "# First test of different reinforcement learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff932811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "import ipywidgets as widgets\n",
    "from ipyevents import Event\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1b8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, str(Path(\"../..\")))\n",
    "import andreas2048\n",
    "from andreas2048.game import *\n",
    "from andreas2048 import gym2048\n",
    "#env = gym.make(\"andreas_2048\")\n",
    "env = gym2048.Env2048()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfceba4",
   "metadata": {},
   "source": [
    "### 1 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b2914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "\n",
    "    def __init__(self, env: gym2048.Env2048) -> None:\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> Action:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def update(self, obs: np.ndarray, action: Action, reward: float, terminated: bool, next_obs):\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_agent(policy: type[Policy], episodes: int = 10000):\n",
    "    scores = []\n",
    "    highest_tiles = []\n",
    "    move_counts = []\n",
    "\n",
    "    target_policy = policy(env)\n",
    "    behaviour_policy = policy(env)\n",
    "\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Start a new hand\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Play one complete hand\n",
    "        while not done:\n",
    "            # Agent chooses action (initially random, gradually more intelligent)\n",
    "            action = agent.get_action(obs)\n",
    "\n",
    "            # Take action and observe result\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Learn from this experience\n",
    "            agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "            # Move to next state\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "        scores.append(env.game.score)\n",
    "        highest_tiles.append(env.game.highest_tile)\n",
    "        move_counts.append(env.game.move_count)\n",
    "\n",
    "    ax1 = plt.subplot()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"axes\", 1.2))\n",
    "    ax1.plot(scores, label=\"Scores\")\n",
    "    p2 = ax2.plot(np.log2(highest_tiles), label=\"Highest tile\", c=\"orange\")\n",
    "    p3 = ax3.plot(move_counts, label=\"Move count\", c=\"green\")\n",
    "    ax1.set_xlabel(\"Episode\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax2.set_ylabel(\"Highest Tile\")\n",
    "    ax3.set_ylabel(\"Move count\")\n",
    "    ax2.yaxis.label.set_color(p2[0].get_color())\n",
    "    ax3.yaxis.label.set_color(p3[0].get_color())\n",
    "    ax2.yaxis.set_major_formatter(FuncFormatter(lambda x, pos: f\"{2**x:n}\"))\n",
    "    ax2.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eafd99b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "STOP",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSTOP\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: STOP"
     ]
    }
   ],
   "source": [
    "raise RuntimeError(\"STOP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f6149",
   "metadata": {},
   "source": [
    "### 2 DQN Policy with a simple neuronal net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2459b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "\n",
    "from andreas2048.game import Action\n",
    "\n",
    "\n",
    "class DQN_Agent(Policy):\n",
    "\n",
    "    def __init__(self, \n",
    "                 env: gym2048.Env2048, \n",
    "                 learning_rate: float = 0.001, \n",
    "                 gamma: float = 0.99,\n",
    "                 epsilon_decay = 0.005,\n",
    "                 epsilon_min = 0.001,\n",
    "            ) -> None:\n",
    "        super().__init__(env)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.rnd = np.random.default_rng()\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> Action:\n",
    "        if self.rnd.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        t = torch.from_numpy(self.env.game.grid_stacks.flatten()).float()\n",
    "        p = torch.nn.functional.softmax(self.model(t), dim=0).detach().numpy()\n",
    "        return self.env.action_space.sample(probability=p)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model =  torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.env.game.grid_stacks.size, out_features=256), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=256, out_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=128,out_features=4)\n",
    "        )\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    def update(self, obs: ndarray, action: Action, reward: float, terminated: bool, next_obs: np.ndarray):\n",
    "        t0 = torch.from_numpy(obs).float()\n",
    "        t1 = torch.from_numpy(next_obs).float()\n",
    "        q0_value: np.ndarray = self.model(t0).detach().numpy()\n",
    "        q1_value: np.ndarray = self.model(t1).detach().numpy()\n",
    "        expected_q_values = reward + self.gamma * next_obs\n",
    "        loss = torch.nn.MSELoss()(t_out, )\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "env.reset()\n",
    "\n",
    "scores = []\n",
    "highest_tiles = []\n",
    "move_counts = []\n",
    "\n",
    "episodes = 100\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "target_update_freq = 10\n",
    "\n",
    "q_policy = DQN_Agent(env)\n",
    "target_policy = DQN_Agent(env)\n",
    "target_policy.model.load_state_dict(q_policy.model.state_dict())\n",
    "optimizer = torch.optim.Adam(q_policy.model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = q_policy.get_action(obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Learn from this experience\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # Move to next state\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    scores.append(env.game.score)\n",
    "    highest_tiles.append(env.game.highest_tile)\n",
    "    move_counts.append(env.game.move_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ded9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(env.game.grid_stacks.flatten()).float()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "n = QNetwork(state_dim=256, action_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n(t), nn.functional.softmax(n(t), dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
